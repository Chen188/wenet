{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c472acc",
   "metadata": {},
   "source": [
    "## Notebook环境\n",
    "Notebook的运行环境可以选择conda_tensorflow_p36，本实验所用的sagemaker版本为2.42.0，接下来我们会安装对应的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install sagemaker==2.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f2c57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role   = get_execution_role()\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a61d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-022346938362'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa57d283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Get https://763104351884.dkr.ecr.us-east-1.amazonaws.com/v2/pytorch-training/manifests/1.8.1-gpu-py38-cu111-ubuntu18.04: no basic auth credentials\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52729205",
   "metadata": {},
   "source": [
    "## 准备Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9757990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.Session().region_name\n",
    "ecr_repository = 'sagemaker-wenet'\n",
    "\n",
    "# 登录ECR服务\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8ef99",
   "metadata": {},
   "source": [
    "### 创建注册表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr create-repository --repository-name $ecr_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1838f",
   "metadata": {},
   "source": [
    "### 构建训练镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "097baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.6.0-gpu-py36-cu110-ubuntu18.04\n",
      "\n",
      "RUN cd / && \\\n",
      "    pip install ninja && \\\n",
      "    apt update && \\\n",
      "    apt-get install sox libsox-dev libsox-fmt-all pkg-config -y && \\\n",
      "    TORCHAUDIO_VERSION=release/0.6 && \\\n",
      "    git clone -b ${TORCHAUDIO_VERSION} https://github.com/pytorch/audio torchaudio && \\\n",
      "    cd torchaudio && \\\n",
      "    pip install .\n",
      "\n",
      "COPY ./requirements.txt /tmp/\n",
      "\n",
      "RUN pip install -r /tmp/requirements.txt && \\\n",
      "    pip install sagemaker-training && \\\n",
      "    apt-get clean\n",
      "    "
     ]
    }
   ],
   "source": [
    "training_docker_file_path = '/fsx/wenet'\n",
    "\n",
    "!cat $training_docker_file_path/Dockerfile-py36-pt1.8.1-cu111-sox-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1abc073d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_repository_uri:  022346938362.dkr.ecr.us-east-1.amazonaws.com/sagemaker-wenet:training-pt181\n",
      "Sending build context to Docker daemon  22.77MB\n",
      "Step 1/4 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n",
      " ---> b0662cb45e61\n",
      "Step 2/4 : RUN cd / &&     pip install ninja &&     apt update &&     apt-get install sox libsox-dev libsox-fmt-all pkg-config -y &&     CUDNN_VERSION=8.0.5.39 && apt-get install cuda-nvrtc-11-1 cuda-nvrtc-dev-11-1 libcudnn8-dev=$CUDNN_VERSION-1+cuda11.1 -y &&     TORCHAUDIO_VERSION=v0.8.1 &&     git clone -b ${TORCHAUDIO_VERSION} https://github.com/pytorch/audio torchaudio &&     cd torchaudio &&     git submodule update --init --recursive &&     pip install .\n",
      " ---> Using cache\n",
      " ---> 38447c6689ad\n",
      "Step 3/4 : COPY ./requirements.txt /tmp/\n",
      " ---> Using cache\n",
      " ---> c5230e1adb12\n",
      "Step 4/4 : RUN pip install -r /tmp/requirements.txt &&     pip install sagemaker-training &&     apt-get clean\n",
      " ---> Using cache\n",
      " ---> e33fdacab042\n",
      "Successfully built e33fdacab042\n",
      "Successfully tagged sagemaker-wenet:training-pt181\n",
      "The push refers to repository [022346938362.dkr.ecr.us-east-1.amazonaws.com/sagemaker-wenet]\n",
      "\n",
      "\u001b[1B52073bbf: Preparing \n",
      "\u001b[1B6c364371: Preparing \n",
      "\u001b[1B4158a735: Preparing \n",
      "\u001b[1B239b61af: Preparing \n",
      "\u001b[1B10f51894: Preparing \n",
      "\u001b[1B7596d9dd: Preparing \n",
      "\u001b[1B958b0384: Preparing \n",
      "\u001b[1Bbd69ecfb: Preparing \n",
      "\u001b[1B1213127b: Preparing \n",
      "\u001b[1Be0d17008: Preparing \n",
      "\u001b[1Ba1f9c5bb: Preparing \n",
      "\u001b[1B0ece5125: Preparing \n",
      "\u001b[1B2a5c2109: Preparing \n",
      "\u001b[1B33244c7a: Preparing \n",
      "\u001b[1B9b4e0c3a: Preparing \n",
      "\u001b[1B8aa56d0f: Preparing \n",
      "\u001b[1B1c2b9fa9: Preparing \n",
      "\u001b[12B58b0384: Waiting g \n",
      "\u001b[1B8351f154: Preparing \n",
      "\u001b[13Bd69ecfb: Waiting g \n",
      "\u001b[1B75dfe93c: Preparing \n",
      "\u001b[14B213127b: Waiting g \n",
      "\u001b[18B596d9dd: Waiting g \n",
      "\u001b[13Bece5125: Waiting g \n",
      "\u001b[1Bd63d7b6b: Preparing \n",
      "\u001b[14Ba5c2109: Waiting g \n",
      "\u001b[14B3244c7a: Waiting g \n",
      "\u001b[14Bb4e0c3a: Waiting g \n",
      "\u001b[1Bab7c290d: Preparing \n",
      "\u001b[1B5d977298: Preparing \n",
      "\u001b[1B2a7c6be4: Preparing \n",
      "\u001b[1B7a6d9152: Preparing \n",
      "\u001b[1B6bb0cefc: Preparing \n",
      "\u001b[1B908e7ec6: Preparing \n",
      "\u001b[20Baa56d0f: Waiting g \n",
      "\u001b[20Bc2b9fa9: Waiting g \n",
      "\u001b[18B451d5d3: Waiting g \n",
      "\u001b[8B2a7c6be4: Pushed   6.821GB/6.797GB\u001b[35A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[38A\u001b[2K\u001b[34A\u001b[2K\u001b[38A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[38A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[36A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[38A\u001b[2K\u001b[30A\u001b[2K\u001b[38A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[38A\u001b[2K\u001b[29A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[38A\u001b[2K\u001b[31A\u001b[2K\u001b[38A\u001b[2K\u001b[31A\u001b[2K\u001b[29A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[30A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[28A\u001b[2K\u001b[29A\u001b[2K\u001b[38A\u001b[2K\u001b[27A\u001b[2K\u001b[36A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[27A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[26A\u001b[2K\u001b[36A\u001b[2K\u001b[26A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[24A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[36A\u001b[2K\u001b[24A\u001b[2KPushing  79.56MB/187.5MB\u001b[24A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[21A\u001b[2K\u001b[36A\u001b[2K\u001b[20A\u001b[2K\u001b[29A\u001b[2K\u001b[20A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[36A\u001b[2K\u001b[22A\u001b[2K\u001b[36A\u001b[2K\u001b[20A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[22A\u001b[2K\u001b[36A\u001b[2K\u001b[22A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[29A\u001b[2KPushing  172.7MB/2.686GB\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[29A\u001b[2K\u001b[24A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[16A\u001b[2K\u001b[17A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[29A\u001b[2K\u001b[15A\u001b[2K\u001b[29A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[29A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[29A\u001b[2K\u001b[15A\u001b[2K\u001b[29A\u001b[2K\u001b[14A\u001b[2K\u001b[29A\u001b[2K\u001b[14A\u001b[2K\u001b[29A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[36A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[36A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[36A\u001b[2K\u001b[10A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[36A\u001b[2K\u001b[10A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2KPushing    556MB/2.126GB\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2KPushing  1.077GB/2.126GB\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[6A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[5A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[4A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[4A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[3A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[1A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[1A\u001b[2K\u001b[36A\u001b[2K\u001b[1A\u001b[2K\u001b[36A\u001b[2K\u001b[1A\u001b[2K\u001b[12A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[1A\u001b[2K\u001b[36A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[36A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2Ktraining-pt181: digest: sha256:7cd60d96e41085ded5359718d19566627df353f56ecd3a31cdc843e25f4c3245 size: 8304\n"
     ]
    }
   ],
   "source": [
    "# 构建训练镜像并推送到ECR\n",
    "tag = ':training-pt181'\n",
    "training_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "print('training_repository_uri: ', training_repository_uri)\n",
    "\n",
    "!cd $training_docker_file_path && docker build -t \"$ecr_repository$tag\" . -f Dockerfile-py36-pt1.8.1-cu111-sox-ready\n",
    "!docker tag {ecr_repository + tag} $training_repository_uri\n",
    "!docker push $training_repository_uri\n",
    "\n",
    "\n",
    "# !docker pull $training_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7695ea",
   "metadata": {},
   "source": [
    "### 构建推理镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "127eedbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ubuntu:latest\n",
      "MAINTAINER <zhendong.peng@mobvoi.com>\n",
      "ENV DEBIAN_FRONTEND=noninteractive\n",
      "RUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list\n",
      "RUN apt-get update && apt-get install -y git cmake wget build-essential\n",
      "RUN git clone https://github.com/mobvoi/wenet.git /home/wenet\n",
      "ARG model=20210327_unified_transformer_exp_server.tar.gz\n",
      "RUN wget -P /home http://mobvoi-speech-public.ufile.ucloud.cn/public/wenet/aishell2/$model\n",
      "RUN tar -xzf /home/$model -C /home\n",
      "ARG build=/home/wenet/runtime/server/x86/build\n",
      "RUN mkdir $build && cmake -S $build/.. -B $build\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoding_docker_file_path='/fsx/wenet/runtime/server/x86'\n",
    "\n",
    "!cat $decoding_docker_file_path/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4273a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建推理容器并推送到ECR\n",
    "tag = ':decoding'\n",
    "decoding_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "print('decoding_repository_uri: ', decoding_repository_uri)\n",
    "\n",
    "\n",
    "!cd $decoding_docker_file_path && docker build -t \"$ecr_repository$tag\" .\n",
    "!docker tag {ecr_repository + tag} $decoding_repository_uri\n",
    "!docker push $decoding_repository_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998e46e",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac3990",
   "metadata": {},
   "source": [
    "### 数据下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /fsx/wenet/examples/aishell/s0 && \\\n",
    "bash run.sh --stage -1 --stop_stage -1 --data /fsx/asr-data/OpenSLR/33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce2f7b",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98876514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "\n",
    "# 指定文件系统的id.\n",
    "file_system_id = 'fs-0f8a3b8eef47b6ff8'\n",
    "# 提供数据集所在的路径，注意格式\n",
    "file_system_path = '/fsx'\n",
    "# 指定挂载文件系统的访问模式，支持\"ro\"（只读）或\"rw\"（读写）两种，注意内置算法只支持 以 ro 的方式挂载\n",
    "file_system_access_mode = 'ro'\n",
    "# 指定文件系统的类型, 支持\"EFS\" 或 \"FSxLustre\"两种.\n",
    "file_system_type = 'FSxLustre'\n",
    "# 以VPC内的方式启动 Amazon SageMaker 训练任务,指定所在子网和安全组，subnet需要为list或者tuple格式\n",
    "security_groups_ids = ['sg-04acfc98f6929ee4e']\n",
    "subnets= ['vpc-3c49de46']\n",
    "\n",
    "# 定义数据输入\n",
    "file_system_input_train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                  file_system_type=file_system_type,\n",
    "                                  directory_path=file_system_path,\n",
    "                                  file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fe551d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a305b99eeb98>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a305b99eeb98>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    bash run.sh --stage 0 --stop_stage 3 --trail_dir /opt/ml/input/data/train --train_set train --data /opt/ml/input/data/33\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd /opt/ml/code/examples/aishell/s0\n",
    "bash run.sh --stage 0 --stop_stage 3 --trail_dir /opt/ml/input/data/train --train_set train --data /opt/ml/input/data/33\n",
    "\n",
    "bash run.sh --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train --train_set train --data /opt/ml/input/data/33\n",
    "\n",
    "\n",
    "bash run.sh --stage 4 --stop_stage 4 --train_set train --trail_dir /opt/ml/input/data/train/sm-train \\\n",
    "    --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --shared_dir /opt/ml/input/data/train/shared\n",
    "\n",
    "\n",
    "networks:\n",
    "  sagemaker-local:\n",
    "    name: sagemaker-local\n",
    "services:\n",
    "  algo-1-un9wk:\n",
    "    command: train\n",
    "    container_name: mfhuhz1akz-algo-1-un9wk\n",
    "    environment:\n",
    "    - AWS_REGION=us-east-1\n",
    "    - TRAINING_JOB_NAME=sagemaker-wenet-2021-06-03-08-49-01-226\n",
    "    image: sagemaker-wenet:training\n",
    "    networks:\n",
    "      sagemaker-local:\n",
    "        aliases:\n",
    "        - algo-1-un9wk\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "    - /tmp/tmplrz93i9x/algo-1-un9wk/output/data:/opt/ml/output/data\n",
    "    - /tmp/tmplrz93i9x/algo-1-un9wk/input:/opt/ml/input\n",
    "    - /tmp/tmplrz93i9x/algo-1-un9wk/output:/opt/ml/output\n",
    "    - /tmp/tmplrz93i9x/model:/opt/ml/model\n",
    "    - /opt/ml/metadata:/opt/ml/metadata\n",
    "    - /fsx/wenet:/opt/ml/code\n",
    "    - /fsx/trail_local_sm:/opt/ml/input/data/train\n",
    "    - /fsx/asr-data/OpenSLR/33:/opt/ml/input/data/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c45566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/wenet\n",
      "Creating dhls9xm95e-algo-1-zvm5j ... \n",
      "Creating dhls9xm95e-algo-1-zvm5j ... done\n",
      "Attaching to dhls9xm95e-algo-1-zvm5j\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m 2021-06-09 13:09:52,717 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m 2021-06-09 13:09:52,730 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m 2021-06-09 13:09:52,742 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m 2021-06-09 13:09:52,753 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m Training Env:\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"wav\": \"/opt/ml/input/data/wav\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     },\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"current_host\": \"algo-1-zvm5j\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"framework_module\": null,\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"algo-1-zvm5j\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     ],\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"train_set\": \"train\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"trail_dir\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"CUDA_VISIBLE_DEVICES\": \"1,2\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     },\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"train\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         },\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"wav\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         }\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     },\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"job_name\": \"sagemaker-wenet-2021-06-09-13-09-47-248\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"master_hostname\": \"algo-1-zvm5j\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-09-13-09-47-248/source/sourcedir.tar.gz\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"module_name\": \"examples/aishell/s0/run1.sh\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"current_host\": \"algo-1-zvm5j\",\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m             \"algo-1-zvm5j\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m         ]\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     },\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m     \"user_entry_point\": \"examples/aishell/s0/run1.sh\"\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m }\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m Environment variables:\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_HOSTS=[\"algo-1-zvm5j\"]\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_HPS={\"CUDA_VISIBLE_DEVICES\":\"1,2\",\"trail_dir\":\"/opt/ml/input/data/train\",\"train_set\":\"train\"}\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_USER_ENTRY_POINT=examples/aishell/s0/run1.sh\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-zvm5j\",\"hosts\":[\"algo-1-zvm5j\"]}\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_INPUT_DATA_CONFIG={\"train\":{\"TrainingInputMode\":\"File\"},\"wav\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_CHANNELS=[\"train\",\"wav\"]\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_CURRENT_HOST=algo-1-zvm5j\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_MODULE_NAME=examples/aishell/s0/run1.sh\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_FRAMEWORK_MODULE=\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-09-13-09-47-248/source/sourcedir.tar.gz\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"wav\":\"/opt/ml/input/data/wav\"},\"current_host\":\"algo-1-zvm5j\",\"framework_module\":null,\"hosts\":[\"algo-1-zvm5j\"],\"hyperparameters\":{\"CUDA_VISIBLE_DEVICES\":\"1,2\",\"trail_dir\":\"/opt/ml/input/data/train\",\"train_set\":\"train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"TrainingInputMode\":\"File\"},\"wav\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-wenet-2021-06-09-13-09-47-248\",\"log_level\":20,\"master_hostname\":\"algo-1-zvm5j\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-09-13-09-47-248/source/sourcedir.tar.gz\",\"module_name\":\"examples/aishell/s0/run1.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-zvm5j\",\"hosts\":[\"algo-1-zvm5j\"]},\"user_entry_point\":\"examples/aishell/s0/run1.sh\"}\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_USER_ARGS=[\"--CUDA_VISIBLE_DEVICES\",\"1,2\",\"--trail_dir\",\"/opt/ml/input/data/train\",\"--train_set\",\"train\"]\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_CHANNEL_WAV=/opt/ml/input/data/wav\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_HP_TRAIN_SET=train\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_HP_TRAIL_DIR=/opt/ml/input/data/train\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m SM_HP_CUDA_VISIBLE_DEVICES=1,2\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m /bin/sh -c ./examples/aishell/s0/run1.sh --CUDA_VISIBLE_DEVICES 1,2 --trail_dir /opt/ml/input/data/train --train_set train\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m \n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m --CUDA_VISIBLE_DEVICES 1,2 --trail_dir /opt/ml/input/data/train --train_set train\n",
      "\u001b[36mdhls9xm95e-algo-1-zvm5j |\u001b[0m 1,2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-558956c753b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'file:///fsx/trail_local_0/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wav'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'file:///fsx/asr-data/OpenSLR/33/data_aishell/wav/'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# estimator.fit(inputs={'train': file_system_input_train})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \"\"\"\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%cd /fsx/wenet\n",
    "\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "# checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, 'common_voice_data', 'checkpoints')\n",
    "# checkpoint_local_path='/opt/ml/checkpoints'\n",
    "\n",
    "hp={'train_set':'train', 'trail_dir':'/opt/ml/input/data/train', 'CUDA_VISIBLE_DEVICES': '1,2'}\n",
    "\n",
    "\n",
    "estimator=PyTorch(\n",
    "    entry_point='examples/aishell/s0/run1.sh',\n",
    "#     image_uri='sagemaker-wenet:training',\n",
    "    image_uri=training_repository_uri,\n",
    "    instance_type='local',\n",
    "    instance_count=1,\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    hyperparameters=hp\n",
    ")\n",
    "\n",
    "estimator.fit({'train':'file:///fsx/trail_local_0/', 'wav':'file:///fsx/asr-data/OpenSLR/33/data_aishell/wav/'})\n",
    "# estimator.fit(inputs={'train': file_system_input_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5f9e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.estimator import TensorFlow\n",
    "\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, 'common_voice_data', 'checkpoints')\n",
    "checkpoint_local_path='/opt/ml/checkpoints'\n",
    "\n",
    "hp={'mode':'train', 'data_dir':'/opt/ml/input/data/training', 'output_dir':'/opt/ml/output', 'batch_size': 64, 'sm_checkpoint': checkpoint_local_path}\n",
    "\n",
    "\n",
    "estimator=TensorFlow(\n",
    "    image_uri=training_repository_uri,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=1,\n",
    "    entry_point='./run_rnnt.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    hyperparameters=hp,\n",
    "    \n",
    "    # Parameters required to enable checkpointing\n",
    "    checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "    checkpoint_local_path=checkpoint_local_path\n",
    ")\n",
    "\n",
    "estimator.fit('s3://sagemaker-us-east-1-022346938362/common_voice_data/cv-corpus-6.1-2020-12-11/zh-CN/tfrecord/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc46e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "# bash run.sh --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train --train_set train --data /opt/ml/input/data/33\n",
    "\n",
    "# 指定文件系统的id.\n",
    "file_system_id = 'fs-0f8a3b8eef47b6ff8'\n",
    "# 提供数据集所在的路径，注意格式\n",
    "file_system_path = '/yobzhbmv'\n",
    "# 指定挂载文件系统的访问模式，支持\"ro\"（只读）或\"rw\"（读写）两种，注意内置算法只支持 以 ro 的方式挂载\n",
    "file_system_access_mode = 'rw'\n",
    "# 指定文件系统的类型, 支持\"EFS\" 或 \"FSxLustre\"两种.\n",
    "file_system_type = 'FSxLustre'\n",
    "# 以VPC内的方式启动 Amazon SageMaker 训练任务,指定所在子网和安全组，subnet需要为list或者tuple格式\n",
    "security_group_ids = ['sg-04acfc98f6929ee4e']\n",
    "# subnets= ['vpc-3c49de46']\n",
    "subnets= ['subnet-07ce0ab63b4cfeb25']\n",
    "\n",
    "# 定义数据输入\n",
    "file_system_input_train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                  file_system_type=file_system_type,\n",
    "                                  directory_path=file_system_path,\n",
    "                                  file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "data_dir   = '/opt/ml/input/data/train/asr-data/OpenSLR/33'\n",
    "trail_dir  = '/opt/ml/input/data/train/sm-train/trail0'\n",
    "shared_dir = '/opt/ml/input/data/train/sm-train/shared'\n",
    "# shared_dir = '/opt/ml/input/data/train/shared'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d236ea",
   "metadata": {},
   "source": [
    "## 数据预处理 - SageMaker托管实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832a25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-08 09:49:56 Starting - Starting the training job...\n",
      "2021-06-08 09:49:58 Starting - Launching requested ML instances......\n",
      "2021-06-08 09:51:11 Starting - Preparing the instances for training......\n",
      "2021-06-08 09:52:06 Downloading - Downloading input data\n",
      "2021-06-08 09:52:06 Training - Downloading the training image...........\u001b[34m2021-06-08 09:54:06,765 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-08 09:54:08,241 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-08 09:54:08,251 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-08 09:54:08,258 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"stop_stage\": 3,\n",
      "        \"data\": \"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\n",
      "        \"stage\": 0,\n",
      "        \"trail_dir\": \"/opt/ml/input/data/train/sm-train/trail0\",\n",
      "        \"train_set\": \"train\",\n",
      "        \"shared_dir\": \"/opt/ml/input/data/train/sm-train/shared\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-wenet-2021-06-08-09-49-53-057\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-08-09-49-53-057/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"examples/aishell/s0/sm-run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"examples/aishell/s0/sm-run.sh\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data\":\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"shared_dir\":\"/opt/ml/input/data/train/sm-train/shared\",\"stage\":0,\"stop_stage\":3,\"trail_dir\":\"/opt/ml/input/data/train/sm-train/trail0\",\"train_set\":\"train\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=examples/aishell/s0/sm-run.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=examples/aishell/s0/sm-run.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-08-09-49-53-057/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data\":\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"shared_dir\":\"/opt/ml/input/data/train/sm-train/shared\",\"stage\":0,\"stop_stage\":3,\"trail_dir\":\"/opt/ml/input/data/train/sm-train/trail0\",\"train_set\":\"train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-wenet-2021-06-08-09-49-53-057\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-08-09-49-53-057/source/sourcedir.tar.gz\",\"module_name\":\"examples/aishell/s0/sm-run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"examples/aishell/s0/sm-run.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data\",\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"--shared_dir\",\"/opt/ml/input/data/train/sm-train/shared\",\"--stage\",\"0\",\"--stop_stage\",\"3\",\"--trail_dir\",\"/opt/ml/input/data/train/sm-train/trail0\",\"--train_set\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=3\u001b[0m\n",
      "\u001b[34mSM_HP_DATA=/opt/ml/input/data/train/asr-data/OpenSLR/33\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIL_DIR=/opt/ml/input/data/train/sm-train/trail0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_SET=train\u001b[0m\n",
      "\u001b[34mSM_HP_SHARED_DIR=/opt/ml/input/data/train/sm-train/shared\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/bin/sh -c ./examples/aishell/s0/sm-run.sh --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --shared_dir /opt/ml/input/data/train/sm-train/shared --stage 0 --stop_stage 3 --trail_dir /opt/ml/input/data/train/sm-train/trail0 --train_set train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mexpr: syntax error: unexpected argument ‘1’\u001b[0m\n",
      "\n",
      "2021-06-08 09:54:05 Training - Training image download completed. Training in progress.\u001b[34mPreparing /opt/ml/input/data/train/sm-train/shared/data/train transcriptions\u001b[0m\n",
      "\u001b[34mPreparing /opt/ml/input/data/train/sm-train/shared/data/dev transcriptions\u001b[0m\n",
      "\u001b[34mPreparing /opt/ml/input/data/train/sm-train/shared/data/test transcriptions\u001b[0m\n",
      "\u001b[34mlocal/aishell_data_prep.sh: AISHELL data preparation succeeded\u001b[0m\n",
      "\u001b[34mprocessed 1000 wavs, 452485 frames\u001b[0m\n",
      "\u001b[34mprocessed 2000 wavs, 906068 frames\u001b[0m\n",
      "\u001b[34mprocessed 3000 wavs, 1353788 frames\u001b[0m\n",
      "\u001b[34mprocessed 4000 wavs, 1795139 frames\u001b[0m\n",
      "\u001b[34mprocessed 5000 wavs, 2244717 frames\u001b[0m\n",
      "\u001b[34mprocessed 6000 wavs, 2694492 frames\u001b[0m\n",
      "\u001b[34mprocessed 7000 wavs, 3144493 frames\u001b[0m\n",
      "\u001b[34mprocessed 8000 wavs, 3592239 frames\u001b[0m\n",
      "\u001b[34mprocessed 9000 wavs, 4045299 frames\u001b[0m\n",
      "\u001b[34mprocessed 10000 wavs, 4491791 frames\u001b[0m\n",
      "\u001b[34mprocessed 11000 wavs, 4938581 frames\u001b[0m\n",
      "\u001b[34mprocessed 12000 wavs, 5391063 frames\u001b[0m\n",
      "\u001b[34mprocessed 13000 wavs, 5840177 frames\u001b[0m\n",
      "\u001b[34mprocessed 14000 wavs, 6291375 frames\u001b[0m\n",
      "\u001b[34mprocessed 15000 wavs, 6733667 frames\u001b[0m\n",
      "\u001b[34mprocessed 16000 wavs, 7188528 frames\u001b[0m\n",
      "\u001b[34mprocessed 17000 wavs, 7644227 frames\u001b[0m\n",
      "\u001b[34mprocessed 18000 wavs, 8091894 frames\u001b[0m\n",
      "\u001b[34mprocessed 19000 wavs, 8539084 frames\u001b[0m\n",
      "\u001b[34mprocessed 20000 wavs, 8993204 frames\u001b[0m\n",
      "\u001b[34mprocessed 21000 wavs, 9438993 frames\u001b[0m\n",
      "\u001b[34mprocessed 22000 wavs, 9888653 frames\u001b[0m\n",
      "\u001b[34mprocessed 23000 wavs, 10335890 frames\u001b[0m\n",
      "\u001b[34mprocessed 24000 wavs, 10789490 frames\u001b[0m\n",
      "\u001b[34mprocessed 25000 wavs, 11237997 frames\u001b[0m\n",
      "\u001b[34mprocessed 26000 wavs, 11680181 frames\u001b[0m\n",
      "\u001b[34mprocessed 27000 wavs, 12126205 frames\u001b[0m\n",
      "\u001b[34mprocessed 28000 wavs, 12574066 frames\u001b[0m\n",
      "\u001b[34mprocessed 29000 wavs, 13028190 frames\u001b[0m\n",
      "\u001b[34mprocessed 30000 wavs, 13480419 frames\u001b[0m\n",
      "\u001b[34mprocessed 31000 wavs, 13941705 frames\u001b[0m\n",
      "\u001b[34mprocessed 32000 wavs, 14389739 frames\u001b[0m\n",
      "\u001b[34mprocessed 33000 wavs, 14845074 frames\u001b[0m\n",
      "\u001b[34mprocessed 34000 wavs, 15296581 frames\u001b[0m\n",
      "\u001b[34mprocessed 35000 wavs, 15744470 frames\u001b[0m\n",
      "\u001b[34mprocessed 36000 wavs, 16188363 frames\u001b[0m\n",
      "\u001b[34mprocessed 37000 wavs, 16644571 frames\u001b[0m\n",
      "\u001b[34mprocessed 38000 wavs, 17092674 frames\u001b[0m\n",
      "\u001b[34mprocessed 39000 wavs, 17546945 frames\u001b[0m\n",
      "\u001b[34mprocessed 40000 wavs, 17996620 frames\u001b[0m\n",
      "\u001b[34mprocessed 41000 wavs, 18440399 frames\u001b[0m\n",
      "\u001b[34mprocessed 42000 wavs, 18884752 frames\u001b[0m\n",
      "\u001b[34mprocessed 43000 wavs, 19334927 frames\u001b[0m\n",
      "\u001b[34mprocessed 44000 wavs, 19786897 frames\u001b[0m\n",
      "\u001b[34mprocessed 45000 wavs, 20238857 frames\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bash run.sh --stage 4 --stop_stage 4 --train_set train  \\\n",
    "    --data /opt/ml/input/data/train/asr-data/OpenSLR/33 \\\n",
    "    --trail_dir /opt/ml/input/data/train/sm-train/trail0 \\\n",
    "    --shared_dir /opt/ml/input/data/train/sm-train/shared \n",
    "\n",
    "# /opt/ml/input/data/train  <==> /fsx\n",
    "# /opt/ml/input/data/train/asr-data/OpenSLR/33  <==> /fsx/asr-data/OpenSLR/33\n",
    "# /opt/ml/input/data/train/sm-train ==> /fsx/sm-train\n",
    "\n",
    "hp= {\n",
    "    'stage': 0, 'stop_stage': 3, 'train_set':'train', \n",
    "    'data': data_dir, 'trail_dir': trail_dir, 'shared_dir': shared_dir\n",
    "}\n",
    "\n",
    "estimator=PyTorch(\n",
    "    entry_point='examples/aishell/s0/sm-run.sh',\n",
    "    image_uri=training_repository_uri,\n",
    "    instance_type='ml.c5.4xlarge',\n",
    "    instance_count=1,\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    hyperparameters=hp,\n",
    "    \n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_group_ids,\n",
    "    \n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True\n",
    ")\n",
    "\n",
    "# estimator.fit({'train':'file:///fsx/trail_local_0/', 'wav':'file:///fsx/asr-data/OpenSLR/33/data_aishell/wav/'})\n",
    "\n",
    "estimator.fit(inputs={'train': file_system_input_train})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f1e4d",
   "metadata": {},
   "source": [
    "## 模型训练 - SageMaker托管实例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bash run.sh --stage 4 --stop_stage 4 --train_set train  \\\n",
    "#     --data /opt/ml/input/data/train/asr-data/OpenSLR/33 \\\n",
    "#     --trail_dir /opt/ml/input/data/train/sm-train/trail0 \\\n",
    "#     --shared_dir /opt/ml/input/data/train/sm-train/shared \n",
    "\n",
    "# instance_type='ml.g4dn.4xlarge'\n",
    "instance_type='ml.p3.2xlarge'\n",
    "instance_count = 2\n",
    "# CUDA_VISIBLE_DEVICES='0,1,2,3'\n",
    "CUDA_VISIBLE_DEVICES='0'\n",
    "\n",
    "hp= {\n",
    "    'stage': 4, 'stop_stage': 4, 'train_set':'train', \n",
    "    'data': data_dir, 'trail_dir': trail_dir, 'shared_dir': shared_dir,\n",
    "    'CUDA_VISIBLE_DEVICES': CUDA_VISIBLE_DEVICES, \n",
    "    'ddp_init_path': '/opt/ml',\n",
    "    'num_nodes': instance_count\n",
    "}\n",
    "\n",
    "estimator=PyTorch( \n",
    "    entry_point='examples/aishell/s0/sm-run.sh',\n",
    "#     image_uri=training_repository_uri,\n",
    "    image_uri='022346938362.dkr.ecr.us-east-1.amazonaws.com/sagemaker-wenet:training',\n",
    "    instance_type =instance_type,\n",
    "    instance_count=instance_count,\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    hyperparameters=hp,\n",
    "    \n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_group_ids,\n",
    "    \n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True\n",
    "    # Parameters required to enable checkpointing\n",
    "#     checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "#     checkpoint_local_path=checkpoint_local_path\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(inputs={'train': file_system_input_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e0a034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/wenet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75e941ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-22 09:22:36 Starting - Starting the training job...\n",
      "2021-06-22 09:22:51 Starting - Launching requested ML instances.........\n",
      "2021-06-22 09:24:33 Starting - Preparing the instances for training............\n",
      "2021-06-22 09:26:15 Downloading - Downloading input data\n",
      "2021-06-22 09:26:15 Training - Downloading the training image...............................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:43,886 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:43,965 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:44,817 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:44,899 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:48,001 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:48,002 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:49,083 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:49,083 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:49,088 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:49,089 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:49,089 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:50,092 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:50,092 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:50,245 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:50,246 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:51,094 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:51,094 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,400 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,401 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,409 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,481 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,481 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,481 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,481 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:51,486 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,105 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,174 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,176 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-06-22 09:31:52,259 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"ddp_init_path\": \"/opt/ml\",\n",
      "        \"stop_stage\": 4,\n",
      "        \"data\": \"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\n",
      "        \"trail_dir\": \"/opt/ml/input/data/train/sm-train/trail0\",\n",
      "        \"train_set\": \"train\",\n",
      "        \"num_nodes\": 2,\n",
      "        \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",\n",
      "        \"stage\": 4,\n",
      "        \"shared_dir\": \"/opt/ml/input/data/train/sm-train/shared\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-wenet-2021-06-22-09-22-33-664\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-22-09-22-33-664/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"examples/aishell/s0/sm-run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"examples/aishell/s0/sm-run.sh\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"CUDA_VISIBLE_DEVICES\":\"0,1,2,3,4,5,6,7\",\"data\":\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"ddp_init_path\":\"/opt/ml\",\"num_nodes\":2,\"shared_dir\":\"/opt/ml/input/data/train/sm-train/shared\",\"stage\":4,\"stop_stage\":4,\"trail_dir\":\"/opt/ml/input/data/train/sm-train/trail0\",\"train_set\":\"train\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=examples/aishell/s0/sm-run.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=examples/aishell/s0/sm-run.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-22-09-22-33-664/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"CUDA_VISIBLE_DEVICES\":\"0,1,2,3,4,5,6,7\",\"data\":\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"ddp_init_path\":\"/opt/ml\",\"num_nodes\":2,\"shared_dir\":\"/opt/ml/input/data/train/sm-train/shared\",\"stage\":4,\"stop_stage\":4,\"trail_dir\":\"/opt/ml/input/data/train/sm-train/trail0\",\"train_set\":\"train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-wenet-2021-06-22-09-22-33-664\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-022346938362/sagemaker-wenet-2021-06-22-09-22-33-664/source/sourcedir.tar.gz\",\"module_name\":\"examples/aishell/s0/sm-run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"examples/aishell/s0/sm-run.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--CUDA_VISIBLE_DEVICES\",\"0,1,2,3,4,5,6,7\",\"--data\",\"/opt/ml/input/data/train/asr-data/OpenSLR/33\",\"--ddp_init_path\",\"/opt/ml\",\"--num_nodes\",\"2\",\"--shared_dir\",\"/opt/ml/input/data/train/sm-train/shared\",\"--stage\",\"4\",\"--stop_stage\",\"4\",\"--trail_dir\",\"/opt/ml/input/data/train/sm-train/trail0\",\"--train_set\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DDP_INIT_PATH=/opt/ml\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_DATA=/opt/ml/input/data/train/asr-data/OpenSLR/33\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIL_DIR=/opt/ml/input/data/train/sm-train/trail0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_SET=train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_NODES=2\u001b[0m\n",
      "\u001b[34mSM_HP_CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_SHARED_DIR=/opt/ml/input/data/train/sm-train/shared\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /bin/sh -c ./examples/aishell/s0/sm-run.sh --CUDA_VISIBLE_DEVICES 0,1,2,3,4,5,6,7 --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --ddp_init_path /opt/ml --num_nodes 2 --shared_dir /opt/ml/input/data/train/sm-train/shared --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train/sm-train/trail0 --train_set train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:53,496 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=54, name='orted', status='sleeping', started='09:31:52')]\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:53,496 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=54, name='orted', status='sleeping', started='09:31:52')]\u001b[0m\n",
      "\u001b[35m2021-06-22 09:31:53,496 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=54, name='orted', status='sleeping', started='09:31:52')]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,13]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,10]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,3]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,13]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,3]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,14]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,9]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,10]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,2]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,15]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,15]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,12]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,12]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,9]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{\"current_host\": \"algo-1\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,0]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{\"current_host\": \"algo-2\", \"hosts\": [\"algo-1\", \"algo-2\"], \"network_interface_name\": \"eth0\"}[1,8]<stdout>:run.sh: init method is file:///opt/ml/ddp_init\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:total gpus is: 16\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:!!!!now we run traing.py\u001b[0m\n",
      "\n",
      "2021-06-22 09:31:44 Training - Training image download completed. Training in progress.\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.148<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Bootstrap : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO NET/Socket : Using [0]eth0:172.31.128.41<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO comm 0x55cc0a51b960 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO comm 0x559ed7d270d0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO comm 0x55c86d6951d0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO comm 0x561f8542fd70 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO comm 0x55a84258b6b0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO comm 0x55d98520bdf0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO comm 0x562a56210030 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO comm 0x55b8e74f5440 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO comm 0x5629a4de3cf0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO comm 0x55ff8c02a970 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO comm 0x560f4b028850 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO comm 0x55fde5d41710 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO comm 0x55988a9351a0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO comm 0x5559995d35c0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO comm 0x56111d647a50 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO comm 0x55acc1432c20 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:488:488 [6] NCCL INFO comm 0x562a58ee2d90 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:547:547 [5] NCCL INFO comm 0x55b8ea1c81a0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:326:326 [5] NCCL INFO comm 0x55599c2a6320 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:552:552 [1] NCCL INFO comm 0x5629a7ab6a50 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:553:553 [6] NCCL INFO comm 0x55acc4105980 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:520:520 [7] NCCL INFO comm 0x56112031a7b0 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:551:551 [4] NCCL INFO comm 0x55988aa0a5d0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:545:545 [1] NCCL INFO comm 0x55cc0d1ee6c0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:473:473 [2] NCCL INFO comm 0x55a84525e410 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO comm 0x55c870367f30 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:343:343 [2] NCCL INFO comm 0x55fde8a14470 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:478:478 [4] NCCL INFO comm 0x561f88102ad0 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:548:548 [7] NCCL INFO comm 0x55d987edeb50 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:638:638 [0] NCCL INFO comm 0x55ff8ecfd6d0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:629 [0] NCCL INFO comm 0x559ed7dfc500 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:550:550 [3] NCCL INFO comm 0x560f4dcfb5b0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,13]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,11]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,9]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,8]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.2.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,15]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,12]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,14]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,10]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:args: \u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: Namespace(checkpoint=None, cmvn='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', config='conf/train_unified_transformer.yaml', cv_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/dev/format.data', dist_backend='nccl', gpu=-1, init_method=None, model_dir='/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer', num_workers=2, pin_memory=True, rank=0, tensorboard_dir='/opt/ml/input/data/train/sm-train/trail0/tensorboard', train_data='/opt/ml/input/data/train/sm-train/trail0/raw_wav/train/format.data', use_amp=False, world_size=-1)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:629:1322 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:the number of model params: 33503762\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-06-22 09:32:08.848 algo-1:545 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-06-22 09:32:08.848 algo-1:473 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-06-22 09:32:08.848 algo-1:547 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-06-22 09:32:08.848 algo-1:629 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-06-22 09:32:08.848 algo-1:488 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-06-22 09:32:08.848 algo-1:548 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-06-22 09:32:08.848 algo-1:478 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-06-22 09:32:08.848 algo-1:160 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-06-22 09:32:08.895 algo-2:552 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-06-22 09:32:08.895 algo-2:343 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-06-22 09:32:08.895 algo-2:520 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-06-22 09:32:08.895 algo-2:638 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-06-22 09:32:08.895 algo-2:551 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-06-22 09:32:08.895 algo-2:326 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-06-22 09:32:08.895 algo-2:550 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-06-22 09:32:08.895 algo-2:553 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-06-22 09:32:08.930 algo-1:548 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-06-22 09:32:08.930 algo-1:545 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-06-22 09:32:08.930 algo-1:473 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-06-22 09:32:08.930 algo-1:547 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-06-22 09:32:08.930 algo-1:629 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-06-22 09:32:08.930 algo-1:160 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-06-22 09:32:08.930 algo-1:488 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-06-22 09:32:08.930 algo-1:478 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-06-22 09:32:08.980 algo-2:552 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-06-22 09:32:08.980 algo-2:343 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-06-22 09:32:08.980 algo-2:520 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-06-22 09:32:08.980 algo-2:638 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-06-22 09:32:08.980 algo-2:551 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-06-22 09:32:08.980 algo-2:326 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-06-22 09:32:08.980 algo-2:550 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-06-22 09:32:08.980 algo-2:553 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m2021-06-22 09:32:19,984 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,172.31.128.41' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:07,397 INFO     [train.py:142] training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:07,397 INFO     [train.py:142] training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:07,398 INFO     [train.py:142] training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:07,408 INFO     [train.py:142] training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:07,421 INFO     [train.py:142] training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:07,433 INFO     [train.py:142] training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:07,438 INFO     [train.py:142] training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:07,441 INFO     [train.py:142] training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:07,452 INFO     [train.py:142] training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:07,456 INFO     [train.py:142] training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:07,475 INFO     [train.py:142] training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:07,493 INFO     [train.py:142] training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:07,492 INFO     [train.py:142] training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:07,516 INFO     [train.py:142] training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:07,527 INFO     [train.py:142] training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:07,584 INFO     [train.py:142] training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:07,913 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:07,915 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:07,919 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:07,921 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:07,921 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:07,936 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:07,949 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:07,950 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:07,951 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:07,951 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:07,954 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:07,957 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:07,959 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:07,963 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:07,978 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,030 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,129 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,130 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,131 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,134 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,136 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,139 INFO     [checkpoint.py:33] Checkpoint: save to checkpoint /opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/init.pt\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,141 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 2, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,142 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,142 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 3, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,143 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,143 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m2021-06-22 09:32:19,979 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /bin/sh -c ./examples/aishell/s0/sm-run.sh --CUDA_VISIBLE_DEVICES 0,1,2,3,4,5,6,7 --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --ddp_init_path /opt/ml --num_nodes 2 --shared_dir /opt/ml/input/data/train/sm-train/shared --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train/sm-train/trail0 --train_set train\"\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,172.31.128.41' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,144 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 1, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 7, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 5, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 6, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,147 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:60: UserWarning: \"sox\" backend is deprecated and will be removed in 0.9.0. Please use \"sox_io\" backend.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  '\"sox\" backend is deprecated and will be removed in 0.9.0. '\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:07,397 INFO     [train.py:142] training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:07,397 INFO     [train.py:142] training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:07,398 INFO     [train.py:142] training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:07,408 INFO     [train.py:142] training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:07,421 INFO     [train.py:142] training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:07,433 INFO     [train.py:142] training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:07,438 INFO     [train.py:142] training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:07,441 INFO     [train.py:142] training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:07,452 INFO     [train.py:142] training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:07,456 INFO     [train.py:142] training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:07,475 INFO     [train.py:142] training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:07,493 INFO     [train.py:142] training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:07,492 INFO     [train.py:142] training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:07,516 INFO     [train.py:142] training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:07,527 INFO     [train.py:142] training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:07,584 INFO     [train.py:142] training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:07,913 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:07,915 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:07,919 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:07,921 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:07,921 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:07,936 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:07,949 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:07,950 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:07,951 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:07,951 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:07,954 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:07,957 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:07,959 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:07,963 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:07,978 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,030 DEBUG    [train.py:220] before if distributed:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,147 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,148 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 4, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 14, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 10, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 9, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 11, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 8, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,129 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,130 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,131 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,134 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,135 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,136 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,139 INFO     [checkpoint.py:33] Checkpoint: save to checkpoint /opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/init.pt\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,141 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,141 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 2, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,142 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,142 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 3, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:08,143 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:08,143 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,144 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 1, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,145 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 7, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 5, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 6, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:08,146 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,145 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,146 DEBUG    [train.py:234] before optimizer\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:08,147 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:08,147 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:08,148 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 13, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 12, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 15, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,154 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 4, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 14, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 10, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 9, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 11, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 8, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,156 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,156 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 13, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 12, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 15, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:08,157 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 0, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,430 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:08,158 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 DEBUG    [train.py:254] before epoch in range\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 INFO     [train.py:259] Epoch 0 TRAIN info lr 8e-08\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,428 INFO     [executor.py:21] {'encoder': 'transformer', 'encoder_conf': {'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True, 'use_dynamic_chunk': True, 'use_dynamic_left_chunk': False}, 'decoder': 'transformer', 'decoder_conf': {'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, 'model_conf': {'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, 'raw_wav': True, 'collate_conf': {'wav_distortion_conf': {'wav_dither': 0.0, 'wav_distortion_rate': 0.0, 'distortion_methods': []}, 'speed_perturb': False, 'feature_extraction_conf': {'feature_type': 'fbank', 'mel_bins': 80, 'frame_shift': 10, 'frame_length': 25, 'using_pitch': False}, 'feature_dither': 0.0, 'spec_aug': True, 'spec_aug_conf': {'warp_for_time': False, 'num_t_mask': 2, 'num_f_mask': 2, 'max_t': 50, 'max_f': 10, 'max_w': 80}}, 'dataset_conf': {'max_length': 40960, 'min_length': 0, 'batch_type': 'static', 'batch_size': 16, 'sort': True}, 'grad_clip': 5, 'accum_grad': 1, 'max_epoch': 180, 'log_interval': 100, 'optim': 'adam', 'optim_conf': {'lr': 0.002}, 'scheduler': 'warmuplr', 'scheduler_conf': {'warmup_steps': 25000}, 'input_dim': 80, 'output_dim': 4233, 'cmvn_file': '/opt/ml/input/data/train/sm-train/trail0/exp/unified_transformer/global_cmvn', 'is_json_cmvn': True, 'rank': 0, 'is_distributed': True, 'use_amp': False}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:08,430 INFO     [executor.py:31] using accumulate grad, new batch size is 1 timeslarger than before\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,583 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/opt/ml/code/wenet/dataset/dataset.py:228: UserWarning: torchaudio.backend.sox_backend.load_wav has been deprecated and will be removed from 0.9.0 release. Please use \"torchaudio.load\".\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  waveform, sample_rate = torchaudio.load_wav(wav_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,583 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,584 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,585 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,588 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,590 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,590 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,592 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,593 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,593 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,597 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,599 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,600 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,608 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,609 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,610 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,612 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,614 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,584 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,585 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:09,586 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,588 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,590 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:09,590 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,592 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,593 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:09,593 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,597 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,599 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:09,600 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,615 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,632 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,634 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,634 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,648 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,648 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,608 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,609 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:09,610 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,612 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,614 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:09,615 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,632 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,634 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:09,634 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,648 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,648 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,658 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,660 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,660 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,662 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,664 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,664 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,667 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,669 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,669 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,671 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:09,650 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,658 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,660 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:09,660 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,662 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,664 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:09,664 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,667 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,673 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,673 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,672 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,675 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,676 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,680 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,682 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,682 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,387 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,387 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,391 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,390 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 441.295959 loss_att 149.722321 loss_ctc 1121.634399 lr 0.00000008 rank 14\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 442.815796 loss_att 143.042419 loss_ctc 1142.286987 lr 0.00000008 rank 12\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,669 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:09,669 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,671 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,673 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:09,673 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,672 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,675 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:09,676 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,680 DEBUG    [executor.py:37] batch_idx: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,682 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:09,682 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,387 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,387 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,391 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 434.094147 loss_att 150.136780 loss_ctc 1096.661377 lr 0.00000008 rank 4\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,395 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 368.494995 loss_att 129.561798 loss_ctc 926.005737 lr 0.00000008 rank 9\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,397 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,397 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,397 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,390 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 441.295959 loss_att 149.722321 loss_ctc 1121.634399 lr 0.00000008 rank 14\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,392 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 442.815796 loss_att 143.042419 loss_ctc 1142.286987 lr 0.00000008 rank 12\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:algo-2: 2021-06-22 09:32:19,393 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,399 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 389.824097 loss_att 134.793030 loss_ctc 984.896606 lr 0.00000008 rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,405 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,406 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,406 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 282.525787 loss_att 95.007523 loss_ctc 720.068359 lr 0.00000008 rank 11\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 312.225250 loss_att 109.350983 loss_ctc 785.598511 lr 0.00000008 rank 8\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 214.814331 loss_att 75.600555 loss_ctc 539.646484 lr 0.00000008 rank 15\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,413 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,413 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,414 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,416 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,417 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,418 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 434.094147 loss_att 150.136780 loss_ctc 1096.661377 lr 0.00000008 rank 4\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,395 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,396 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 368.494995 loss_att 129.561798 loss_ctc 926.005737 lr 0.00000008 rank 9\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,397 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:algo-1: 2021-06-22 09:32:19,397 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,396 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:algo-2: 2021-06-22 09:32:19,397 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,399 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,404 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 389.824097 loss_att 134.793030 loss_ctc 984.896606 lr 0.00000008 rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:algo-1: 2021-06-22 09:32:19,405 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,405 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,406 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,406 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 303.280518 loss_att 103.404678 loss_ctc 769.657471 lr 0.00000008 rank 3\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,421 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,421 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 289.595398 loss_att 102.165825 loss_ctc 726.931091 lr 0.00000008 rank 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 282.525787 loss_att 95.007523 loss_ctc 720.068359 lr 0.00000008 rank 11\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,411 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 312.225250 loss_att 109.350983 loss_ctc 785.598511 lr 0.00000008 rank 8\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 214.814331 loss_att 75.600555 loss_ctc 539.646484 lr 0.00000008 rank 15\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,423 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,423 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 230.996857 loss_att 81.526962 loss_ctc 579.759888 lr 0.00000008 rank 7\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 302.437317 loss_att 104.895615 loss_ctc 763.367920 lr 0.00000008 rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 230.754364 loss_att 80.489830 loss_ctc 581.371582 lr 0.00000008 rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,426 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,426 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,430 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,412 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,413 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:algo-2: 2021-06-22 09:32:19,413 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,414 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,416 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,417 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,418 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,419 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 303.280518 loss_att 103.404678 loss_ctc 769.657471 lr 0.00000008 rank 3\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,430 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,431 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 229.802505 loss_att 78.364456 loss_ctc 583.157959 lr 0.00000008 rank 2\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,431 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,432 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,432 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,461 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,472 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,472 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,473 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 252.985519 loss_att 90.151047 loss_ctc 632.932617 lr 0.00000008 rank 10\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:algo-1: 2021-06-22 09:32:19,420 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,421 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,421 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 289.595398 loss_att 102.165825 loss_ctc 726.931091 lr 0.00000008 rank 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:algo-1: 2021-06-22 09:32:19,422 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,423 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,423 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 230.996857 loss_att 81.526962 loss_ctc 579.759888 lr 0.00000008 rank 7\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,424 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 302.437317 loss_att 104.895615 loss_ctc 763.367920 lr 0.00000008 rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 230.754364 loss_att 80.489830 loss_ctc 581.371582 lr 0.00000008 rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,473 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,474 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,474 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,475 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 263.134216 loss_att 94.944244 loss_ctc 655.577515 lr 0.00000008 rank 13\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,487 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,488 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,488 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb4349a9980]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 1] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fb4345e4fb7]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 2] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fb4345e6921]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 3] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fb43462f967]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 4] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fb4346369da]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 5] /lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7fb43463e284]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 6] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fb2d79198f4]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 7] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fb2d791803e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 8] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fb2d791623c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 9] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fb2d79146cd]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [10] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fb2d7912f77]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:algo-1: 2021-06-22 09:32:19,425 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,426 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:algo-1: 2021-06-22 09:32:19,426 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,430 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,430 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,431 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 229.802505 loss_att 78.364456 loss_ctc 583.157959 lr 0.00000008 rank 2\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,431 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,432 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:algo-1: 2021-06-22 09:32:19,432 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,461 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,472 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,472 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,473 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 252.985519 loss_att 90.151047 loss_ctc 632.932617 lr 0.00000008 rank 10\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,473 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,474 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:algo-2: 2021-06-22 09:32:19,474 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,475 DEBUG    [executor.py:99] BEFORE optimizer.zero_grad(). idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:103] AFTER self.step += 1 , idx_grad: 0\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:105] BEFORE if (batch_idx % log_interval == 0) or True:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,486 DEBUG    [executor.py:116] TRAIN Batch 0/470 loss 263.134216 loss_att 94.944244 loss_ctc 655.577515 lr 0.00000008 rank 13\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [11] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7fb2d791162c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [12] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7fb2d796dc1e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [13] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7fb2d795634c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [14] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7fb2d795470e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [15] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7fb2d79515d6]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [16] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7fb2d794dcce]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [17] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7fb2d794a0f6]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [18] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7fb2d7944fcc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [19] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7fb2d793ffed]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [20] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7fb2d793bafb]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [21] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7fb2d7932e9a]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,487 DEBUG    [executor.py:37] batch_idx: 1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,488 DEBUG    [executor.py:58] BEFORE with context():\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:algo-2: 2021-06-22 09:32:19,488 DEBUG    [executor.py:71] IN loss, loss_att, loss_ctc = model(feats, # no amp\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb4349a9980]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 1] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fb4345e4fb7]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 2] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fb4345e6921]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 3] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fb43462f967]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 4] [1,14]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fb4346369da]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 5] /lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7fb43463e284]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 6] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fb2d79198f4]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 7] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fb2d791803e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 8] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fb2d791623c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [ 9] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fb2d79146cd]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [10] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fb2d7912f77]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [11] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7fb2d791162c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [12] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7fb2d796dc1e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [22] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fb2d7c00608]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [13] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7fb2d795634c]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [14] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7fb2d795470e]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [15] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7fb2d79515d6]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [16] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7fb2d794dcce]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [17] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7fb2d794a0f6]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [18] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7fb2d7944fcc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [19] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7fb2d793ffed]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [20] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7fb2d793bafb]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [21] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7fb2d7932e9a]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [22] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fb2d7c00608]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [23] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fb2d7c57921]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [1,14]<stderr>:[24] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fb2d7c4c8fc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [25] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fb2d7c3d34f]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [26] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fb2d7c3d5d7]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [27] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fb2d7c137ff]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [28] [1,14]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55acba7b0ed4]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [29] [1,14]<stderr>:python(+0x199afc)[0x55acba838afc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f7b6c4bb980]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [1,9]<stderr>:[ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f7b6c0f6fb7]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 2] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f7b6c0f8921]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 3] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f7b6c141967]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 4] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f7b6c1489da]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 5] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f7b6c150284]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 6] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f7a0f42b8f4]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [23] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fb2d7c57921]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [1,14]<stderr>:[24] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fb2d7c4c8fc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [25] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fb2d7c3d34f]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [26] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fb2d7c3d5d7]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [27] [1,14]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fb2d7c137ff]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [28] [1,14]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55acba7b0ed4]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] [29] [1,14]<stderr>:python(+0x199afc)[0x55acba838afc]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:[algo-2:00553] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f7b6c4bb980]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [1,9]<stderr>:[ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f7b6c0f6fb7]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 2] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f7b6c0f8921]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 3] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f7b6c141967]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 4] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f7b6c1489da]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 5] [1,9]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f7b6c150284]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 6] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f7a0f42b8f4]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 7] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f7a0f42a03e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 8] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f7a0f42823c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 9] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f7a0f4266cd]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [10] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f7a0f424f77]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [11] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f7a0f42362c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [12] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f7a0f47fc1e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [13] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f7a0f46834c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [14] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f7a0f46670e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [15] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f7a0f4635d6]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 7] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f7a0f42a03e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 8] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f7a0f42823c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [ 9] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f7a0f4266cd]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [10] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f7a0f424f77]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [11] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f7a0f42362c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [12] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f7a0f47fc1e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [13] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f7a0f46834c]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [14] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f7a0f46670e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [15] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f7a0f4635d6]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [16] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f7a0f45fcce]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [17] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f7a0f45c0f6]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [18] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f7a0f456fcc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [19] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f7a0f451fed]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [20] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f7a0f44dafb]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [21] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f7a0f444e9a]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [22] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f7a0f712608]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [23] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f7a0f769921]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [16] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f7a0f45fcce]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [17] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f7a0f45c0f6]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [18] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f7a0f456fcc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [19] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f7a0f451fed]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [20] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f7a0f44dafb]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [21] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f7a0f444e9a]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [22] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f7a0f712608]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [23] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f7a0f769921]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [24] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f7a0f75e8fc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [25] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f7a0f74f34f]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [26] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f7a0f74f5d7]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [24] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f7a0f75e8fc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [25] [1,9]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f7a0f74f34f]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [26] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f7a0f74f5d7]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [27] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f7a0f7257ff]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [28] [1,9]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x56299e8d6ed4]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [29] [1,9]<stderr>:python(+0x199afc)[0x56299e95eafc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f8f423f3980]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 1] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f8f4202efb7]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 2] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f8f42030921]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 3] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f8f42079967]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 4] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f8f420809da]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 5] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f8f42088284]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 6] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f8de53638f4]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 7] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f8de536203e]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 8] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f8de536023c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 9] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f8de535e6cd]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [10] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f8de535cf77]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [11] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f8de535b62c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [12] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f8de53b7c1e]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [13] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f8de53a034c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [14] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f8de539e70e]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [27] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f7a0f7257ff]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [28] [1,9]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x56299e8d6ed4]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] [29] [1,9]<stderr>:python(+0x199afc)[0x56299e95eafc]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:[algo-2:00552] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f8f423f3980]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 1] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f8f4202efb7]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 2] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f8f42030921]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 3] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f8f42079967]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 4] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f8f420809da]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [15] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f8de539b5d6]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [16] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f8de5397cce]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [17] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f8de53940f6]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [18] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f8de538efcc]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [19] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f8de5389fed]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [20] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f8de5385afb]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [21] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f8de537ce9a]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [22] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f8de564a608]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [23] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f8de56a1921]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [24] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f8de56968fc]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [25] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f8de568734f]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [26] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f8de56875d7]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [27] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f8de565d7ff]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [28] [1,0]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x559ed090ced4]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [29] [1,0]<stderr>:python(+0x199afc)[0x559ed0994afc]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f07d4317980]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 1] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f07d3f52fb7]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 2] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f07d3f54921]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 3] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f07d3f9d967]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 4] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f07d3fa49da]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 5] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f07d3fac284]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 5] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f8f42088284]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 6] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f8de53638f4]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 7] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f8de536203e]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 8] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f8de536023c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [ 9] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f8de535e6cd]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [10] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f8de535cf77]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [11] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f8de535b62c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [12] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f8de53b7c1e]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [13] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f8de53a034c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [14] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f8de539e70e]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [15] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f8de539b5d6]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [16] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f8de5397cce]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [17] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f8de53940f6]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 6] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f06772878f4]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 7] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f067728603e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 8] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f067728423c]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 9] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f06772826cd]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [10] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f0677280f77]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [11] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f067727f62c]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [12] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f06772dbc1e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [13] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f06772c434c]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [18] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f8de538efcc]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [19] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f8de5389fed]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [20] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f8de5385afb]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [21] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f8de537ce9a]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [22] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f8de564a608]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [23] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f8de56a1921]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [24] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f8de56968fc]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [25] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f8de568734f]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [26] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f8de56875d7]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [27] [1,0]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f8de565d7ff]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [28] [1,0]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x559ed090ced4]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] [29] [1,0]<stderr>:python(+0x199afc)[0x559ed0994afc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [14] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f06772c270e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [15] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f06772bf5d6]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [16] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f06772bbcce]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [17] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f06772b80f6]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [18] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f06772b2fcc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [19] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f06772adfed]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [20] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f06772a9afb]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [21] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f06772a0e9a]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [22] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f067756e608]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [23] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f06775c5921]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [24] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f06775ba8fc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [25] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f06775ab34f]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [26] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f06775ab5d7]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [27] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f06775817ff]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [28] [1,3]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55c8670d0ed4]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [29] [1,3]<stderr>:python(+0x199afc)[0x55c867158afc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f24a9d30980]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 1] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f24a996bfb7]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 2] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f24a996d921]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 3] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f24a99b6967]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 4] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f24a99bd9da]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 5] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f24a99c5284]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 6] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f234cca08f4]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 7] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f234cc9f03e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 8] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f234cc9d23c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 9] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f234cc9b6cd]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [10] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f234cc99f77]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [11] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f234cc9862c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [12] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f234ccf4c1e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [13] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f234ccdd34c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [14] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f234ccdb70e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [15] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f234ccd85d6]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [16] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f234ccd4cce]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00629] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f07d4317980]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 1] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f07d3f52fb7]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 2] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f07d3f54921]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 3] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f07d3f9d967]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 4] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f07d3fa49da]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 5] [1,3]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f07d3fac284]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 6] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f06772878f4]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 7] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f067728603e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 8] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f067728423c]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [ 9] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f06772826cd]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [10] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f0677280f77]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [11] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f067727f62c]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [12] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f06772dbc1e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [13] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f06772c434c]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [14] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f06772c270e]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [15] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f06772bf5d6]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [16] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f06772bbcce]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [17] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f06772b80f6]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [18] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f06772b2fcc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [19] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f06772adfed]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [20] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f06772a9afb]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [21] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f06772a0e9a]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [22] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f067756e608]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [23] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f06775c5921]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [24] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f06775ba8fc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [25] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f06775ab34f]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [26] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f06775ab5d7]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [27] [1,3]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f06775817ff]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [28] [1,3]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55c8670d0ed4]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] [29] [1,3]<stderr>:python(+0x199afc)[0x55c867158afc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [17] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f234ccd10f6]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [18] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f234cccbfcc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [19] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f234ccc6fed]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [20] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f234ccc2afb]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [21] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f234ccb9e9a]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [22] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f234cf87608]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [23] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f234cfde921]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [24] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f234cfd38fc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [25] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f234cfc434f]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [26] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f234cfc45d7]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [27] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f234cf9a7ff]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [28] [1,1]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55cc0434bed4]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [29] [1,1]<stderr>:python(+0x199afc)[0x55cc043d3afc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fe176db5980]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe1769f0fb7]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 2] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe1769f2921]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 3] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fe176a3b967]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fe176a429da]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 5] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7fe176a49fbc]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00160] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f24a9d30980]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 1] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f24a996bfb7]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 2] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f24a996d921]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 3] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f24a99b6967]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 4] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f24a99bd9da]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 5] [1,1]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f24a99c5284]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 6] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f234cca08f4]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 7] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f234cc9f03e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 8] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f234cc9d23c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [ 9] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f234cc9b6cd]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [10] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f234cc99f77]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [11] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f234cc9862c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [12] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f234ccf4c1e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [13] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f234ccdd34c]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [14] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f234ccdb70e]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [15] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f234ccd85d6]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [16] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f234ccd4cce]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [17] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f234ccd10f6]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [18] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f234cccbfcc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [19] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f234ccc6fed]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [20] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f234ccc2afb]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [21] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f234ccb9e9a]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [22] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f234cf87608]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [23] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f234cfde921]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 6] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fe019d258f4]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 7] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fe019d2403e]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 8] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fe019d2223c]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 9] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fe019d206cd]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [10] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fe019d1ef77]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [11] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7fe019d1ea90]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [12] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7fe019d1d4ff]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [13] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7fe019d3ec82]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [14] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fe01a00c608]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [15] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fe01a063921]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [16] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fe01a0588fc]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fe01a04934f]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [18] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fe01a0495d7]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [1,15]<stderr>:[19] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fe01a01f7ff]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [24] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f234cfd38fc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [25] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f234cfc434f]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [20] [1,15]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x561117517ed4]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [21] [1,15]<stderr>:python(+0x199afc)[0x56111759fafc]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [22] [1,15]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x5611175c22ca]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [23] [1,15]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x56111759adb6]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [24] [1,15]<stderr>:python(+0x1956a6)[0x56111759b6a6]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [25] [1,15]<stderr>:python(PyObject_Call+0x3e)[0x561117517cde]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [26] [1,15]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x5611175c3952]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [27] python(+0x192f66)[0x561117598f66]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [1,15]<stderr>:[28] python(+0x193c61)[0x561117599c61]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [29] [1,15]<stderr>:python(+0x199bd5)[0x56111759fbd5]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f5bd629d980]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 1] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f5bd5ed8fb7]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [26] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f234cfc45d7]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [27] [1,1]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f234cf9a7ff]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [28] [1,1]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x55cc0434bed4]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] [29] [1,1]<stderr>:python(+0x199afc)[0x55cc043d3afc]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00545] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fe176db5980]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe1769f0fb7]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 2] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe1769f2921]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 2] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f5bd5eda921]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 3] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f5bd5f23967]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 4] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f5bd5f2a9da]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 5] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f5bd5f32284]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 6] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f5a7920d8f4]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 7] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f5a7920c03e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 8] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f5a7920a23c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 9] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f5a792086cd]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [10] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f5a79206f77]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 3] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fe176a3b967]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fe176a429da]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 5] [1,15]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7fe176a49fbc]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 6] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fe019d258f4]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 7] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fe019d2403e]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 8] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fe019d2223c]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [ 9] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fe019d206cd]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [10] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fe019d1ef77]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [11] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7fe019d1ea90]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [12] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7fe019d1d4ff]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [13] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7fe019d3ec82]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [14] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fe01a00c608]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [15] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fe01a063921]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [16] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fe01a0588fc]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fe01a04934f]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [18] [1,15]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fe01a0495d7]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [1,15]<stderr>:[19] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fe01a01f7ff]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [20] [1,15]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x561117517ed4]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [21] [1,15]<stderr>:python(+0x199afc)[0x56111759fafc]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [22] [1,15]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x5611175c22ca]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [23] [1,15]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x56111759adb6]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [24] [1,15]<stderr>:python(+0x1956a6)[0x56111759b6a6]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [25] [1,15]<stderr>:python(PyObject_Call+0x3e)[0x561117517cde]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [26] [1,15]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x5611175c3952]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [27] python(+0x192f66)[0x561117598f66]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [1,15]<stderr>:[28] python(+0x193c61)[0x561117599c61]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] [29] [1,15]<stderr>:python(+0x199bd5)[0x56111759fbd5]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:[algo-2:00520] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [11] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f5a7920562c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [12] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f5a79261c1e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [13] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f5a7924a34c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [14] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f5a7924870e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [15] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f5a792455d6]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [16] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f5a79241cce]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [17] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f5a7923e0f6]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [18] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f5a79238fcc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [19] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f5a79233fed]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [20] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f5a7922fafb]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:corrupted size vs. prev_size\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f5bd629d980]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 1] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f5bd5ed8fb7]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 2] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f5bd5eda921]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 3] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f5bd5f23967]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 4] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f5bd5f2a9da]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 5] [1,6]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x854)[0x7f5bd5f32284]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 6] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f5a7920d8f4]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 7] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f5a7920c03e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 8] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f5a7920a23c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [ 9] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f5a792086cd]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [10] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f5a79206f77]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [11] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0xad8)[0x7f5a7920562c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [12] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN17TensorFusionStageC1ESt6vectorI10BufferTaskSaIS1_EERS0_IP9SemaphoreSaIS5_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISB_EESt4hashIiESt8equal_toIiESaISt4pairIKiSE_EEEPPvP11CUstream_stRSt5mutex+0x11e)[0x7f5a79261c1e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [13] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorI17TensorFusionStageE9constructIS1_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvPT_DpOT0_+0x112)[0x7f5a7924a34c]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [21] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f5a79226e9a]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [22] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f5a794f4608]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [23] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f5a7954b921]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [24] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f5a795408fc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [25] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f5a7953134f]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [26] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f5a795315d7]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [1,6]<stderr>:[27] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f5a795077ff]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [1,6]<stderr>:[28] [1,6]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x562a5024bed4]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [29] [1,6]<stderr>:python(+0x199afc)[0x562a502d3afc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [14] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaI17TensorFusionStageEE9constructIS0_JRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEEvRS1_PT_DpOT0_+0x9c)[0x7f5a7924870e]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [15] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt23_Sp_counted_ptr_inplaceI17TensorFusionStageSaIS0_ELN9__gnu_cxx12_Lock_policyE2EEC1IJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEES1_DpOT_+0x128)[0x7f5a792455d6]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [16] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2I17TensorFusionStageSaIS4_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagPT_RKT0_DpOT1_+0x182)[0x7f5a79241cce]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 0] [1,10]<stderr>:/lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb1f2681980]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fb1f22bcfb7]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 2] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fb1f22be921]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 3] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fb1f2307967]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [17] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12__shared_ptrI17TensorFusionStageLN9__gnu_cxx12_Lock_policyE2EEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS7_EERS6_IP9SemaphoreSaISC_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISI_EESt4hashIiESt8equal_toIiESaISt4pairIKiSL_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xc0)[0x7f5a7923e0f6]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [18] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt10shared_ptrI17TensorFusionStageEC2ISaIS0_EJRSt6vectorI10BufferTaskSaIS5_EERS4_IP9SemaphoreSaISA_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISG_EESt4hashIiESt8equal_toIiESaISt4pairIKiSJ_EEEPPvRP11CUstream_stRSt5mutexEEESt19_Sp_make_shared_tagRKT_DpOT0_+0xae)[0x7f5a79238fcc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [19] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt15allocate_sharedI17TensorFusionStageSaIS0_EJRSt6vectorI10BufferTaskSaIS3_EERS2_IP9SemaphoreSaIS8_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISE_EESt4hashIiESt8equal_toIiESaISt4pairIKiSH_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_ERKT0_DpOT1_+0xca)[0x7f5a79233fed]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [20] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZSt11make_sharedI17TensorFusionStageJRSt6vectorI10BufferTaskSaIS2_EERS1_IP9SemaphoreSaIS7_EERSt13unordered_mapIiSt10unique_ptrI4TaskSt14default_deleteISD_EESt4hashIiESt8equal_toIiESaISt4pairIKiSG_EEEPPvRP11CUstream_stRSt5mutexEESt10shared_ptrIT_EDpOT0_+0xc7)[0x7f5a7922fafb]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [21] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x748)[0x7f5a79226e9a]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [22] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f5a794f4608]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [23] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f5a7954b921]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [24] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f5a795408fc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [25] [1,6]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f5a7953134f]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [26] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f5a795315d7]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [1,6]<stderr>:[27] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f5a795077ff]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [1,6]<stderr>:[28] [1,6]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x562a5024bed4]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] [29] [1,6]<stderr>:python(+0x199afc)[0x562a502d3afc]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-1:00488] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 0] [1,10]<stderr>:/lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb1f2681980]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fb1f22bcfb7]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 2] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fb1f22be921]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 3] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7fb1f2307967]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 4] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fb1f230e9da]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 5] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7fb1f2315fbc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 6] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fb0955f18f4]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 7] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fb0955f003e]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 8] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fb0955ee23c]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 4] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7fb1f230e9da]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 5] [1,10]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7fb1f2315fbc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 6] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7fb0955f18f4]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 7] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7fb0955f003e]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 8] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7fb0955ee23c]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 9] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fb0955ec6cd]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [ 9] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7fb0955ec6cd]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [10] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fb0955eaf77]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [11] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7fb0955eaa90]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [12] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7fb0955e94ff]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [13] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7fb09560ac82]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [14] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fb0958d8608]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [15] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fb09592f921]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [16] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fb0959248fc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fb09591534f]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [18] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fb0959155d7]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [19] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fb0958eb7ff]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [20] python(_PyCFunction_FastCallDict+0x154)[0x55fddf20eed4]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [21] [1,10]<stderr>:python(+0x199afc)[0x55fddf296afc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [22] [1,10]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x55fddf2b92ca]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [23] [1,10]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x55fddf291db6]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [24] [1,10]<stderr>:python(+0x1956a6)[0x55fddf2926a6]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [25] [1,10]<stderr>:python(PyObject_Call+0x3e)[0x55fddf20ecde]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [26] [1,10]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x55fddf2ba952]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [27] [1,10]<stderr>:python(+0x192f66)[0x55fddf28ff66]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [28] [1,10]<stderr>:python(+0x193c61)[0x55fddf290c61]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [29] python(+0x199bd5)[0x55fddf296bd5]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [10] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7fb0955eaf77]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [11] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7fb0955eaa90]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [12] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7fb0955e94ff]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [13] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7fb09560ac82]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 0] [1,11]<stderr>:/lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f8c2a1e8980]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f8c29e23fb7]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [1,11]<stderr>:[ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f8c29e25921]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 3] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f8c29e6e967]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 4] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f8c29e759da]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 5] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7f8c29e7cfbc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 6] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f8acd1588f4]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 7] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f8acd15703e]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 8] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f8acd15523c]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 9] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f8acd1536cd]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [14] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7fb0958d8608]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [10] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f8acd151f77]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [11] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7f8acd151a90]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [12] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7f8acd1504ff]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [15] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7fb09592f921]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [13] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7f8acd171c82]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [14] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f8acd43f608]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [15] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f8acd496921]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [16] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f8acd48b8fc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f8acd47c34f]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [18] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f8acd47c5d7]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [19] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f8acd4527ff]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [20] [1,11]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x560f440efed4]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [21] [1,11]<stderr>:python(+0x199afc)[0x560f44177afc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [22] [1,11]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x560f4419a2ca]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [23] [1,11]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x560f44172db6]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [24] [1,11]<stderr>:python(+0x1956a6)[0x560f441736a6]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [25] [1,11]<stderr>:python(PyObject_Call+0x3e)[0x560f440efcde]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [26] [1,11]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x560f4419b952]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [27] python(+0x192f66)[0x560f44170f66]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [28] [1,11]<stderr>:python(+0x193c61)[0x560f44171c61]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [29] python(+0x199bd5)[0x560f44177bd5]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:run.sh: line 206:   553 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:run.sh: line 206:   478 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:run.sh: line 206:   550 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:run.sh: line 206:   552 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:run.sh: line 206:   520 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:run.sh: line 206:   638 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:run.sh: line 206:   343 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:run.sh: line 206:   551 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:run.sh: line 206:   326 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real has exited due to process rank 14 with PID 78 on\u001b[0m\n",
      "\u001b[34mnode algo-2 exiting improperly. There are three reasons this could occur:\n",
      "\u001b[0m\n",
      "\u001b[34m1. this process did not call \"init\" before exiting, but others in\u001b[0m\n",
      "\u001b[34mthe job did. This can cause a job to hang indefinitely while it waits\u001b[0m\n",
      "\u001b[34mfor all processes to call \"init\". By rule, if one process calls \"init\",\u001b[0m\n",
      "\u001b[34mthen ALL processes must call \"init\" prior to termination.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [16] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7fb0959248fc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7fb09591534f]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [18] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7fb0959155d7]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [19] [1,10]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7fb0958eb7ff]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [20] python(_PyCFunction_FastCallDict+0x154)[0x55fddf20eed4]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [21] [1,10]<stderr>:python(+0x199afc)[0x55fddf296afc]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [22] [1,10]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x55fddf2b92ca]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [23] [1,10]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x55fddf291db6]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [24] [1,10]<stderr>:python(+0x1956a6)[0x55fddf2926a6]\n",
      "\u001b[0m\n",
      "\u001b[34m2. this process called \"init\", but exited without calling \"finalize\".\u001b[0m\n",
      "\u001b[34mBy rule, all processes that call \"init\" MUST call \"finalize\" prior to\u001b[0m\n",
      "\u001b[34mexiting or it will be considered an \"abnormal termination\"\n",
      "\u001b[0m\n",
      "\u001b[34m3. this process called \"MPI_Abort\" or \"orte_abort\" and the mca parameter\u001b[0m\n",
      "\u001b[34morte_create_session_dirs is set to false. In this case, the run-time cannot\u001b[0m\n",
      "\u001b[34mdetect that the abort call was an abnormal termination. Hence, the only\u001b[0m\n",
      "\u001b[34merror message you will receive is this one.\n",
      "\u001b[0m\n",
      "\u001b[34mThis may have caused other processes in the application to be\u001b[0m\n",
      "\u001b[34mterminated by signals sent by mpirun.real (as reported here).\n",
      "\u001b[0m\n",
      "\u001b[34mYou can avoid this message by specifying -quiet on the mpirun.real command line.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\n",
      "\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [25] [1,10]<stderr>:python(PyObject_Call+0x3e)[0x55fddf20ecde]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [26] [1,10]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x55fddf2ba952]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [27] [1,10]<stderr>:python(+0x192f66)[0x55fddf28ff66]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [28] [1,10]<stderr>:python(+0x193c61)[0x55fddf290c61]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] [29] python(+0x199bd5)[0x55fddf296bd5]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:[algo-2:00343] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:munmap_chunk(): invalid pointer\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] *** Process received signal ***\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] Signal: Aborted (6)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] Signal code:  (-6)\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 0] [1,11]<stderr>:/lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f8c2a1e8980]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f8c29e23fb7]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [1,11]<stderr>:[ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f8c29e25921]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 3] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x89967)[0x7f8c29e6e967]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 4] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x909da)[0x7f8c29e759da]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 5] [1,11]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(cfree+0x58c)[0x7f8c29e7cfbc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 6] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim+0x20)[0x7f8acd1588f4]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 7] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt16allocator_traitsISaIiEE10deallocateERS0_Pim+0x2b)[0x7f8acd15703e]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 8] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEE13_M_deallocateEPim+0x32)[0x7f8acd15523c]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [ 9] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt12_Vector_baseIiSaIiEED2Ev+0x41)[0x7f8acd1536cd]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [10] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZNSt6vectorIiSaIiEED2Ev+0x41)[0x7f8acd151f77]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [11] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN6BucketD1Ev+0x1c)[0x7f8acd151a90]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [12] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13BufferManagerC2ESt6vectorI10BufferTaskSaIS1_EEmm+0x9ab)[0x7f8acd1504ff]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [13] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/libsmddpcpp.so(_ZN13HerringClient9allReduceEPKvPvi17herringDataType_tiiiP11CUstream_stSt8functionIFvvEE+0x530)[0x7f8acd171c82]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [14] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x31608)[0x7f8acd43f608]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [15] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x88921)[0x7f8acd496921]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [16] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x7d8fc)[0x7f8acd48b8fc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [17] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e34f)[0x7f8acd47c34f]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [18] [1,11]<stderr>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x6e5d7)[0x7f8acd47c5d7]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [19] /opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib/smddpcommon.cpython-36m-x86_64-linux-gnu.so(+0x447ff)[0x7f8acd4527ff]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [20] [1,11]<stderr>:python(_PyCFunction_FastCallDict+0x154)[0x560f440efed4]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [21] [1,11]<stderr>:python(+0x199afc)[0x560f44177afc]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [22] [1,11]<stderr>:python(_PyEval_EvalFrameDefault+0x30a)[0x560f4419a2ca]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [23] [1,11]<stderr>:python(PyEval_EvalCodeEx+0x966)[0x560f44172db6]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [24] [1,11]<stderr>:python(+0x1956a6)[0x560f441736a6]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [25] [1,11]<stderr>:python(PyObject_Call+0x3e)[0x560f440efcde]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [26] [1,11]<stderr>:python(_PyEval_EvalFrameDefault+0x1992)[0x560f4419b952]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [27] python(+0x192f66)[0x560f44170f66]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [28] [1,11]<stderr>:python(+0x193c61)[0x560f44171c61]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] [29] python(+0x199bd5)[0x560f44177bd5]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:[algo-2:00550] *** End of error message ***\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:run.sh: line 206:   553 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:run.sh: line 206:   478 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:run.sh: line 206:   550 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:run.sh: line 206:   552 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:run.sh: line 206:   520 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:run.sh: line 206:   638 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:run.sh: line 206:   343 Aborted                 python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:run.sh: line 206:   551 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:run.sh: line 206:   326 Segmentation fault      python wenet/bin/train.py --config $train_config --train_data $feat_dir/$train_set/format.data --cv_data $feat_dir/dev/format.data ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --num_workers 2 --tensorboard_dir $tensorboard_dir $cmvn_opts --pin_memory\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real has exited due to process rank 14 with PID 78 on\u001b[0m\n",
      "\u001b[34mnode algo-2 exiting improperly. There are three reasons this could occur:\n",
      "\u001b[0m\n",
      "\u001b[34m1. this process did not call \"init\" before exiting, but others in\u001b[0m\n",
      "\u001b[34mthe job did. This can cause a job to hang indefinitely while it waits\u001b[0m\n",
      "\u001b[34mfor all processes to call \"init\". By rule, if one process calls \"init\",\u001b[0m\n",
      "\u001b[34mthen ALL processes must call \"init\" prior to termination.\n",
      "\u001b[0m\n",
      "\u001b[34m2. this process called \"init\", but exited without calling \"finalize\".\u001b[0m\n",
      "\u001b[34mBy rule, all processes that call \"init\" MUST call \"finalize\" prior to\u001b[0m\n",
      "\u001b[34mexiting or it will be considered an \"abnormal termination\"\n",
      "\u001b[0m\n",
      "\u001b[34m3. this process called \"MPI_Abort\" or \"orte_abort\" and the mca parameter\u001b[0m\n",
      "\u001b[34morte_create_session_dirs is set to false. In this case, the run-time cannot\u001b[0m\n",
      "\u001b[34mdetect that the abort call was an abnormal termination. Hence, the only\u001b[0m\n",
      "\u001b[34merror message you will receive is this one.\n",
      "\u001b[0m\n",
      "\u001b[34mThis may have caused other processes in the application to be\u001b[0m\n",
      "\u001b[34mterminated by signals sent by mpirun.real (as reported here).\n",
      "\u001b[0m\n",
      "\u001b[34mYou can avoid this message by specifying -quiet on the mpirun.real command line.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "2021-06-22 09:32:31 Uploading - Uploading generated training model\n",
      "2021-06-22 09:32:31 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sagemaker-wenet-2021-06-22-09-22-33-664: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /bin/sh -c ./examples/aishell/s0/sm-run.sh --CUDA_VISIBLE_DEVICES 0,1,2,3,4,5,6,7 --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --ddp_init_path /opt/ml --num_nodes 2 --shared_dir /opt/ml/input/data/train/sm-train/shared --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train/sm-t",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-140e7e22bb17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_system_input_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3681\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3683\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 ),\n\u001b[1;32m   3244\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             )\n\u001b[1;32m   3247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sagemaker-wenet-2021-06-22-09-22-33-664: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /bin/sh -c ./examples/aishell/s0/sm-run.sh --CUDA_VISIBLE_DEVICES 0,1,2,3,4,5,6,7 --data /opt/ml/input/data/train/asr-data/OpenSLR/33 --ddp_init_path /opt/ml --num_nodes 2 --shared_dir /opt/ml/input/data/train/sm-train/shared --stage 4 --stop_stage 4 --trail_dir /opt/ml/input/data/train/sm-t"
     ]
    }
   ],
   "source": [
    "\n",
    "# bash run.sh --stage 4 --stop_stage 4 --train_set train  \\\n",
    "#     --data /opt/ml/input/data/train/asr-data/OpenSLR/33 \\\n",
    "#     --trail_dir /opt/ml/input/data/train/sm-train/trail0 \\\n",
    "#     --shared_dir /opt/ml/input/data/train/sm-train/shared \n",
    "\n",
    "\n",
    "data_dir   = '/opt/ml/input/data/train/asr-data/OpenSLR/33'\n",
    "trail_dir  = '/opt/ml/input/data/train/sm-train/trail0'\n",
    "shared_dir = '/opt/ml/input/data/train/sm-train/shared'\n",
    "\n",
    "# instance_type='ml.g4dn.4xlarge'\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count = 2\n",
    "# CUDA_VISIBLE_DEVICES='0,1,2,3'\n",
    "CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7'\n",
    "\n",
    "hp= {\n",
    "    'stage': 4, 'stop_stage': 4, 'train_set':'train', \n",
    "    'data': data_dir, 'trail_dir': trail_dir, 'shared_dir': shared_dir,\n",
    "    'CUDA_VISIBLE_DEVICES': CUDA_VISIBLE_DEVICES, \n",
    "    'ddp_init_path': '/opt/ml',\n",
    "    'num_nodes': instance_count\n",
    "}\n",
    "\n",
    "estimator=PyTorch( \n",
    "    entry_point='examples/aishell/s0/sm-run.sh',\n",
    "#     image_uri=training_repository_uri,\n",
    "    image_uri='022346938362.dkr.ecr.us-east-1.amazonaws.com/sagemaker-wenet:training-pt181',\n",
    "    instance_type =instance_type,\n",
    "    instance_count=instance_count,\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    hyperparameters=hp,\n",
    "    \n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_group_ids,\n",
    "    \n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    distribution = {\n",
    "        'smdistributed':{\n",
    "            'dataparallel':{\n",
    "                'enabled': True, \n",
    "#                 \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Parameters required to enable checkpointing\n",
    "#     checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "#     checkpoint_local_path=checkpoint_local_path\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(inputs={'train': file_system_input_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70499d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
